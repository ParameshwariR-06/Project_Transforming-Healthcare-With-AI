{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-16T21:34:04.951302Z","iopub.execute_input":"2023-07-16T21:34:04.951964Z","iopub.status.idle":"2023-07-16T21:34:04.988768Z","shell.execute_reply.started":"2023-07-16T21:34:04.951919Z","shell.execute_reply":"2023-07-16T21:34:04.987582Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\n/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\n/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":" # <center>Embracing the Technological Revolution: How AI is Reshaping Healthcare?</center>","metadata":{}},{"cell_type":"markdown","source":"It's a known fact that the entire world saw a tremendous shift in the Artificial Intelligence (AI) industry in the last couple of years. It won't be wrong to say that we all are wrapped around by its hype. \"NextGen Invent Corp\" in one of their blogs titled \"AI in Surgery: Opportunities, Applications, and Challenges\" mentioned that the market that once valued 600 million dollars in 2014 is expected to touch 150 billion dollars by 2026[[1]](https://nextgeninvent.com/blogs/ai-in-surgery-opportunities-applications-and-challenges/). \n\nHowever, in the midst of the pandemic, one particular sector the \"healthcare\", grabbed significant attention from all over the globe. AI appeared to be that beacon of hope when merged with medicine, is capable of reshaping healthcare by revolutionizing our understanding of diagnoses and disease treatments. The incorporation of AI in  healthcare can change the future of human well-being by improving and enhancing patient outcomes and healthcare practices.\n\nIn 2019, the Wolrd Health Organization (WHO)[[2]](https://www.who.int/) reported death of 55.4 million people worldwide[[3]](https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death). \nIt's alarming to know that 55% of these deaths were caused by :\n1. Heart diseases, \n2. Strokes, \n3. Chronic pulmonary diseases, \n4. Respiratory infections, and \n5. Neonatal conditions. \n\nIt does not end her, what's equally concerning is the fact that the report claims additional 15.3 million deaths which were unreported by any government or health agency, leaving their causes unknown.[[3]](https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death).\n\nAs COVID-19 [[4]](https://www.who.int/health-topics/coronavirus#tab=tab_1) emerged as a pandemic it swiftly became one of the leading causes of death attributing to approximately 15.5 million lives worldwide. The lockdown that followed worldwide resulted in compromising access to healthcare leading to 74% of total deaths caused in 2020 on a global-scale due to non-communicable diseases. With so many lives suffering, and everyone in the world hoped and prayed to be free from pandemic by the year-end, the havoc did not stop as 2021 came along with another wave of COVID leading to 15.2 million deaths worldwide by the pandemic alone.\n\nAlthough the numbers reported seem disturbing, the emergence of AI in healthcare offers hope to each one of us. The integration of machine learning, deep learning algorithms and robotics presents extraordinary opportunities to revolutionize healthcare practices across various domains. The article explores how transformative power of AI in healthcare can emphasize the innovative applications accelerating this technological revolution. The article particularly focuses on two key sectors: Surgery and Drug Discovery. By discussing the advancements made in recent years, this article aim to highlight the importance of AI in revolutionizing healthcare, and how the world is paving the way for a future where lives would be saved, diseases would be better understood, and healthcare would becomes more accessible. The article further explores how Kaggle has launched its highest number of healthcare competitions in the past two years indicating how AI in healthcare is becoming pivotal.","metadata":{}},{"cell_type":"markdown","source":"# <center> PART I : \"AI in Surgery -  An Emerging Field\"</center>\n","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src=\"https://emag.medicalexpo.com/wp-content/uploads/sites/9/surgical-robots.jpg\" height=\"400\" width=\"700\"/></center>\n<br>\n<center>Figure - 1: Image Source: <i><a href=\"https://emag.medicalexpo.com/real-time-artificial-intelligence-system-for-the-operating-room/\">E-Magazine</a></i></center>\n<br>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Surgery is a procedure that's been practiced since ancient times. It is a critical area in medicine. Even with numerous technological advancements over the years, it still carries inherent risks and complexities. A variety of factors giving rise to these complications include human errors, unexpected patient reactions, or equipment malfunctions, leading to adverse outcomes and even fatalities.\n\nAccording to the article \"Failure Rates in Surgery,\"[[20]](https://medium.com/thinq-at-ucla/failure-rates-in-surgery-89ca73250b57) a staggering number of approximately 310 million people undergo surgery each year. Disturbingly, about 50 million of these patients experience postoperative complications, and approximately 1.5 million individuals succumb to these complications. Bacterial infections around the surgical wound and cardiovascular issues, including heart attacks account for the most common complications.\n\nWith so many lives at stakes and the potential for human error to contribute to these outcomes, it becomes essential to explore innovative solutions incorporating AI to mitigate risks and enhance patient safety.\n\nContrary to common misconceptions that AI in surgery solely involves surgical robots, its applications are much broader and encompass three distinct phases of surgical procedures described as follows:\n\n**Pre-operative Phase:**\nThe phase exactly prior to the surgery where decisions such as treatment-planning, and risk assessment are taken to prevent fatal outcomes is referred to as the Pre-operative Phase. In this phase, doctors/ surgeons can  utlize AI for analyzing vast amount of patient data collected from medical history, diagnostic tests, imaging results to identify potential risks and optimize treatment plans. Machine learning algorithms can help predict patient outcomes and also recommend personalized approaches to mitigate the complications and improve patient outcomes.\n\n**Intra-Operative Phase:**\nWhen the actual surgery takes place in the operation theatre, it is referred to as intra-operative phase. The coming together of surgical team and AI could provide real-time decision support, playing a pivotal role during this phase. AI, surgical instruments and imaging technologies together can help surgeons use additional  information during the operation, for instance: highlighting critical areas/structures that are otherwise difficult to recognize, suggesting optimal surgical pathways to mitigate risks, or alerting the surgical team regarding the potential complications beforehand. This would lead improved surgical precision, reduced risk of errors, and more successful surgeries.\n\n**Post-Operative Phase:**\n\nWhen the surgery has been performed and the patient is monitored to keep a check on any complications that might arise. AI can aid the task by analyzing vital signs and help identify early signs of complications. This would enable surgeons to intervene on time, preventing serious complications beforehand that might have arised after the surgery.\n\nAccording to Forbes, the AI in surgery market is expected to reach $225.4 million in 2024, indicating the growing interest and investment in this transformative technology.**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Recent Advancements","metadata":{}},{"cell_type":"markdown","source":"# 1. AI and Brain Tumor Surgery.","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src=\"https://www.researchgate.net/publication/355236851/figure/fig2/AS:1085665110425601@1635854141237/Potential-clinical-impacts-of-AI-in-the-neurosurgical-management-of-brain-tumours-in-the.png\" height=\"600\" width=\"900\"/></center>\n<br>\n<center>Figure - 2: Image Source: <i><a href=\"https://www.researchgate.net/publication/355236851_Artificial_Intelligence_in_Brain_Tumour_Surgery-An_Emerging_Paradigm\">Williams et al.</a></i></center>\n<br>\n","metadata":{}},{"cell_type":"markdown","source":"**1. Pre-Operative Phase**\n\n**Screening and Diagnosis:**\n\n1. ML algorithms: ML algorithms are developed for the analysis of routine blood tests to predict the presence of brain tumors.It can lead to identify subtle changes in blood markers associated with brain tumors which are difficult to recognize otherwise. The researchers performed a study using routine blood tests from 15,176 neurological patients to develop a machine learning predictive model for diagnosing brain tumors. The model was validated by analyzing the blood tests of 68 brain tumor patients and 215 control patients who visited the neurological emergency service. Only patients with head imaging and routine blood test data were included in the validation. The model showed a sensitivity of 96% and specificity of 74% in the validation group. This research demonstrates the potential of using machine learning and routine blood tests for diagnosing brain tumors. The accuracy of this approach is comparable to and possibly complements traditional imaging studies, offering a new avenue for diagnosing these serious neurological diseases and utilizing valuable information from routine blood tests [[5]](https://www.nature.com/articles/s41598-019-51147-3).\n\n2. AI-powered detection tools: Glioblastoma is considered to be one of the most aggressive primary brain tumor. The risks associated with invasive procedures and the need for repeated biopsies makes it challenging to diagnose and monitor the tumor. A study developed a novel method for detecting glioblastoma using plasma denaturation profiles obtained through differential scanning fluorimetry. The researchers utilized machine learning algorithms to distinguish between denaturation profiles of blood samples from 84 glioma patients and 63 healthy controls, achieving an accuracy of 92%. This workflow has the potential to be applied to various types of cancer, offering a minimally invasive diagnostic and monitoring tool through a simple blood test.\nDifferential scanning fluorometry of blood samples combined with AI has shown high accuracy in detecting specific brain tumor types, such as glioblastoma. This technology may enable risk stratification and guide management decisions [[6]](https://www.mdpi.com/2072-6694/13/6/1294)\n\n**Radiological Imaging:**\n1. Natural Language Processing (NLP) ML algorithms: NLP algorithms have been developed by Brown et al.[[7]](https://academic.oup.com/jamia/article/25/5/568/4569611?login=false) to interpret MRI brain requests and select the most appropriate imaging sequences. This study introduces a machine learning method for automatically determining the appropriate MRI sequences based on the unstructured text of clinical indications and patient demographics in MRI orders. Three machine learning models were compared to a baseline model, and the gradient boosting machine model outperformed the others, achieving high accuracy (95%), precision (86%), recall (80%), and low Hamming loss (0.0487). This research demonstrates the feasibility of using machine learning to automate sequence selection in MRI orders, which has implications for improving the safety, quality, and cost-effectiveness of medical imaging services.These algorithms outperform radiologists in sequence selection, reducing errors and maximizing clinical applicability.\n\n2. Radiomics and ML: Radiomics analyzes three-dimensional radiological images to detect subtle patterns that are indicative of brain tumors. ML algorithms applied to radiomics have been successful in characterizing tumor molecular expression [[8]](https://link.springer.com/article/10.1007/s11548-017-1691-5), detecting central nervous system metastases, differentiating between primary and metastatic CNS lesions, predicting tumor grade, and identifying genetic mutations[[9]](https://ieeexplore.ieee.org/document/8375811/). These algorithms often outperform human radiologists in accuracy and efficiency.\n\n**Prognostication and Risk Stratification:**\n1. Artificial Neural Networks (ANNs): ANNs have been used to predict survival outcomes in patients with brain tumors. By integrating patient data, such as age, tumor type, and presence of systemic disease, ANNs can provide accurate one-year survival predictions. These models have shown superior performance compared to traditional scoring systems.\n2. Individualized Predictions: ML platforms have been developed to predict the progression and recurrence of brain tumors, such as meningiomas, based solely on radiological data. These models enable individualized predictions, supporting personalized treatment decisions.\n\n**Operative Planning:**\n1. AI-assisted Decision-making: ML algorithms can aid in surgical planning by providing insights into tumor characteristics and helping clinicians make decisions between biopsy and resection. Deep learning algorithms have shown advancements in predicting molecular characteristics, such as IDH1 mutation, from imaging data, potentially reducing the need for invasive biopsies.\n2. Enhanced Accuracy: Deep learning models have exhibited superior accuracy in detecting brain metastases compared to conventional ML algorithms. These advancements contribute to improved surgical planning and patient outcomes.\n\n\n**2. Intraoperative Phase**\n1. Image-Guided Surgery: AI algorithms have been developed to assist surgeons during brain tumor resection. Image-guided surgery combines preoperative imaging data with real-time intraoperative imaging to provide enhanced visualization and navigation. AI algorithms can register and fuse these images, allowing surgeons to precisely target and remove tumors while minimizing damage to healthy tissue.\n2. Augmented Reality (AR) and Virtual Reality (VR): AR and VR technologies are being integrated into intraoperative workflows. These technologies provide surgeons with interactive, three-dimensional visualizations of the brain and tumor, aiding in surgical planning, guidance, and enhancing precision during the procedure.\n\n**3. Postoperative Phase**\n1. Pathological Assessment: AI algorithms can assist in the analysis of postoperative histopathological slides. By automating tasks such as tumor segmentation, grading, and identification of specific molecular markers, AI algorithms can speed up the analysis process and provide more accurate and consistent results.\n2. Treatment Response Prediction: AI models can analyze postoperative imaging data to predict treatment response and identify potential tumor recurrence. These models utilize machine learning and deep learning techniques to detect subtle changes in radiological images over time and assist in making informed decisions regarding further treatment strategies.\n3. Patient Monitoring and Follow-up: AI algorithms can be employed to monitor patients' postoperative recovery and track their long-term outcomes. By analyzing various data sources, including medical records, imaging data, and patient-reported outcomes, AI models can provide personalized monitoring, early detection of complications, and long-term prognostic information.\n\nAdvancements in AI have significant implications for pre-operative, intraoperative and postoperative brain tumor management. From image-guided surgery and augmented reality visualization to automated pathological assessment and treatment response prediction, AI algorithms can enhance surgical precision, improve postoperative analysis, and aid in personalized patient monitoring. ","metadata":{}},{"cell_type":"markdown","source":"\n# 2. Smart Tissue Autonomous Robot (STAR): Robot Performs first laparoscopic surgery without human help.\n","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src=\"https://www.science.org/cms/10.1126/scirobotics.abj2908/asset/fe6b7b8a-5152-4001-9234-2a713435bc82/assets/images/large/scirobotics.abj2908-f2.jpg\" height=\"700\" width=\"900\"/></center>\n<br>\n<center>Figure - 3 : Image Source: <i><a href=\"https://www.science.org/doi/10.1126/scirobotics.abj2908\">Tissue motion tracking.</a></i></center>\n<br>\n","metadata":{}},{"cell_type":"markdown","source":"The researchers at Johns Hopkins University developed an autonomous robot called STAR [[10]](https://www.science.org/doi/10.1126/scirobotics.abj2908) that performs laparoscopic anastomosis, a surgical procedure to stitch together parts of the small bowel.They made several advancements to increase the robot's autonomy during surgery. The first improvement is a tissue-tracking system that uses a machine learning algorithm based on Convolutional Neural Networks (CNNs) [[11]](https://ieeexplore.ieee.org/document/8308186). This algorithm tracks the tissue's movement caused by breathing and other factors during laparoscopic surgery. By analyzing the position history of special markers on the tissue, the algorithm accurately predicts the tissue's motion and helps the robot synchronize its movements accordingly.The system also detects the tissue's breathing motion and deformations. This information is used to alert the surgeon to initiate a replanning step, allowing adjustments to the surgical approach as needed.\nTo understand how motion of the tissue is tracked,let's dive deeper. The researchers developed a convolutional neural networks (CNNs) (40) based algorithm. \"A total of 9294 examples constituting varying motion profiles (4317 for the breathing examples and 4977 for the stopped breathing examples) [[10]](https://www.science.org/doi/10.1126/scirobotics.abj2908) were collected. The position history of the NIR markers in the last 2 sec and the direction of motion of the markers between now and 2 s ago are fed as an input to the CNN (2 channels of 128 x 128 pixels downsampled frames (see examples in Fig. 3A). The CNN is comprised of  4 convolutional layers, 3 dense layers, and 2 outputs (for moving/stopped). The activation function used for convolutional layers and the first 3 dense layers is Rectified Linear Unit (ReLU), and SoftMax for the final dense layer (for labeling the moving and not moving states). The network predicted the motion with 93.56% training accuracy. The results were used for motion synchronization of robot with tissue and trigger data collection and planning the right times (e.g., on the bottom of breathing cycles) to improve the control algorithm accuracy\"[[10]](https://www.science.org/doi/10.1126/scirobotics.abj2908).\n\nAnother enhancement is the robot's ability to detect tool failures promptly. If any of the robot's tools stop working, the system can quickly detect it and take appropriate action to prevent issues during the surgery.The researchers also incorporated autonomous camera motion control, which allows the robot to adjust the camera's view for better visualization during the surgery.Suture planning is another aspect where autonomy is improved. The system suggests different suture plans with uniform and nonuniform spacing, giving the surgeon options to choose from or approve a replanning step.\nTo improve accuracy, the system pre-filters the suture plan to reduce noise and irregularity. This helps in achieving precise suture placement.\nAdditionally, the system predicts tool collisions with the tissue, minimizing potential risks during the procedure.\n\nTo achieve these autonomous features CNNs were employed for motion tracking and breathing motion detection, which proved to be more robust and accurate compared to traditional methods like optical flow.\nCompared to previous works, this enhanced autonomy significantly increases the percentage of sutures completed autonomously, reducing the workload for the surgeon. The system's effectiveness was evaluated in both phantom experiments and in vivo survival studies using porcine models for laparoscopic bowel anastomosis. The study showed the potential to standardize surgical outcomes, regardless of surgeons' training and experience. By improving autonomy and reducing the surgeon's workload, the system enhanced the consistency and quality of surgery, leading to better patient outcomes and reduced complications.These advancements offer significant benefits to patients and surgeons.\n","metadata":{}},{"cell_type":"markdown","source":"# 3. JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS).","metadata":{}},{"cell_type":"markdown","source":"What's important to note here is as much importance is given to the ML/DL algorithms, given the lack of data it's equally important to develop datasets specially to revolutinize surgical procedures. And one such research was conducted by the researchers at Johns Hopkins. They published a paper titled \"JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) \"[[12]](https://cirl.lcsr.jhu.edu/wp-content/uploads/2015/11/JIGSAWS.pdf) introducing a dataset specifically designed for analyzing surgical activities. \nThe dataset is composed of video recordings of surgeons performing various surgical tasks using the da Vinci Surgical System (a robotic surgical platform known for minimally invasive procedures.)\n\nThe JIGSAWS dataset comprises of of eight expert surgeons recordings while they conduct 3 surgical tasks: **suturing, knot-tying, and needle-passing.** These tasks were performed on a bench-top model, simulating a realistic surgical environment. The dataset captured the fine-grained hand movements, tool usage, and surgical actions involved in each task providing us with a rich collection of multimodal data,\n\nIt does not stop here, in addition to the raw data, the author annotated the dataset to provide ground truth information for various aspects of surgical gestures and skills. They labeling of the recorded data was done manually with temporal boundaries for individual gestures, like picking up a tool or suturing, enabling researchers to analyze and classify different surgical actions accurately.\n\nFurthermore, the JIGSAWS dataset includes assessments of surgical skills based on expert evaluations. Expert surgeons independently assessed the performance of each task using established rating scales, such as the Objective Structured Assessment of Technical Skills (OSATS) [[13]](https://pubmed.ncbi.nlm.nih.gov/9052454/) and the Global Rating Score (GRS). These skill assessments provide an objective measure of surgical proficiency and can be utilized to evaluate and compare the performance of different surgeons or surgical techniques.\n\nTo facilitate analysis and modeling of the dataset, the authors also provide toolkits for hidden Markov models (HMMs)[[14]](https://link.springer.com/chapter/10.1007/978-3-642-30618-1_17) and linear dynamical systems (LDS). These toolkits assist researchers in developing algorithms and models to automatically recognize and assess surgical gestures and skills from the recorded data.\n\nThe primary objective of creating the JIGSAWS dataset is to support research in the field of surgical robotics and skill acquisition. The dataset allows researchers to investigate various aspects of surgical performance, including motion analysis, skill assessment, gesture recognition, and training evaluation. The availability of such a comprehensive dataset can potentially lead to advancements in surgical robotics, surgical training, and the development of intelligent systems to assist surgeons in their decision-making processes and skill development.\n\nIn summary, the JIGSAWS dataset presented in the paper provides a valuable resource for studying surgical activities and assessing surgical skills. The dataset, along with the accompanying annotations and analysis tools, aims to promote research and innovation in the field of surgical robotics and contribute to improved surgical training and performance evaluation.","metadata":{}},{"cell_type":"markdown","source":"# 4. Automation of surgical skill assessment using a three-stage machine learning algorithm.","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-021-84295-6/MediaObjects/41598_2021_84295_Fig2_HTML.png?as=webp\" height=\"1100\" width=\"1100\"/></center>\n<br>\n<center>Figure - 4: Image Source: <i><a href=\"https://www.nature.com/articles/s41598-021-84295-6\">The Feature Pyramid Network (FPN) based Faster R-CNN.</i></center>\n<br>\n\n","metadata":{}},{"cell_type":"markdown","source":"The study [[15]](https://www.nature.com/articles/s41598-021-84295-6) proposes a three-stage machine learning method for automating surgical skill assessment in laparoscopic cholecystectomy videos. The goal is to improve surgical skills and reduce adverse outcomes by providing continuous training and feedback to surgeons. The three stages of the proposed method are as follows:\n\nInstrument Detection: A Convolutional Neural Network (CNN) is trained to identify and localize surgical instruments in video frames. The CNN-based classifier, known as Resnet50-FPN, uses a Feature Pyramid Network (FPN) architecture for object detection. It addresses the challenge of detecting objects at different scales by combining multi-scale feature maps from different layers of a convolutional neural network. FPN incorporates both a bottom-up pathway and a top-down pathway. The bottom-up pathway processes the input image to generate feature maps at different resolutions, while the top-down pathway combines these maps using upsampling and lateral connections to create a feature pyramid that preserves fine-grained details and semantic information. FPN improves the accuracy and robustness of object detection by effectively handling objects of varying sizes in an image.\nThe network takes as input an image of arbitrary size and outputs bounding boxes for each detected instrument along with a class label (grasper or clipper) and confidence score.\n\nMotion Feature Extraction: The instrument location predictions from the previous stage are used to compute motion features. These features capture the surgeon's instrument handling skills over time. The instrument trajectories are analyzed to calculate motion-related metrics.\n\nSurgical Skill Prediction: A linear regression model is trained using the extracted motion features as input to predict surgical skills. The model learns the relationship between the motion features and the skill ratings provided by expert surgeons. The aim is to distinguish between good and poor surgical skill based on the predicted skill scores.\n\nThe proposed three-stage modeling approach achieved an accuracy of 87 ± 0.2% in distinguishing good versus poor surgical skill. However, it should be noted that the technique cannot reliably quantify the degree of surgical skill. Nevertheless, it represents an important advancement towards the automation of surgical skill assessment.\n\nThe study acknowledges that traditional methods of assessing surgical skills, such as direct observation by experts or retrospective analysis of operation videos, have limitations in terms of reproducibility and rater availability. Therefore, the use of machine learning algorithms, specifically tailored to laparoscopic videos, holds promise for objective and automated skill assessment in surgical practice.\n\nThe dataset used in the study consisted of 242 laparoscopic cholecystectomy videos, segmented into procedural phases. Clip applications at the end of the hepatocystic dissection phase were annotated and served as the proxy for surgical skill. The clips were rated by four board-certified surgeons using a Likert scale. The inter-rater reliability of the skill ratings was calculated to be 79%.\n\nOverall, the proposed method addresses the need for automated surgical skill assessment and provides a framework for evaluating laparoscopic cholecystectomy videos. By combining instrument detection, motion feature extraction, and skill prediction, the method offers a potential solution for continuous surgical training and feedback, leading to improved patient outcomes.","metadata":{}},{"cell_type":"markdown","source":"# 5. Expert surgeons and deep learning models can predict the outcome of surgical hemorrhage from 1 min of video.","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-11549-2/MediaObjects/41598_2022_11549_Fig1_HTML.png?as=webp\" height=\"900\" width=\"900\"/></center>\n<br>\n<center>Figure - 5: Image Source: <i><a href=\"https://www.nature.com/articles/s41598-022-11549-2\">SOCALNet architecture.</i></center>\n<br>\n\n","metadata":{}},{"cell_type":"markdown","source":"\n\n\nSOCALNet [[16]](https://www.nature.com/articles/s41598-022-11549-2) ,  is a  deep learning neural network (DNN) developed by researchers to predict the ability to control hemorrhage during minimally invasive surgery leveraging only the first minute of surgical video. \nThe model is trained on the SOCAL dataset, which contains 147 videos of surgeons managing hemorrhage in a cadaveric simulator used to predict blood loss and task success in critical hemorrhage control task. \n\nSOCALNet captures the video in individual frames as shown in Figur 5.(A). SOCAL images are used to finetune a pre-trained ResNet convolutional neural network(CNN) to predict blood loss and task success for each individual frame. \n\nTweaks: Removal of the penultimate layer and a 1 × 4 matrix of values to predict success/failure or BL were obtained (The processs repeated for all frames resulting in generating a new matrix where N (number of frames) rows and 4 columns).\n\nThe output matrix from Figure 5. (B) and the information Figure 5.(C) [e.g. ‘Is suction present? Yes (check); is Muscle present? No (X), etc.; encoded as 8 binary values per frame (Nx8)]' is fed as an input into a temporal layer. \nTemporal layer: Long-short-term memory (LSTM) modified recurrent neural network allows for temporal analysis across all frames. \n\nThe 2D matrix : features generated from the ResNet and information for instance ('is suction present') retrieved from each frame is fed into the Temporal Layer. \nAll LSTM predictions are consolidated in one dense layer resulting in a final prediction of success/failure, and blood loss (in mL) is output.\n\nThe four professionally-trained skull-base neurosurgery instructors are shown the first minute for 20 videos. The experts independently assessed surgeon skill, predicted outcome, and estimated blood loss. The performance of expert assessments was taken and when compared with the SOCALNet the results showed the superior performance of SOCALNet compared to human experts. \n\n\nSOCALNet achieved a sesitivity of 100% and specificity of 66% by correctly predicting the outcome and identifying successful attempts in 17 out of 20 trials.The positive predictive value (PPV) attained was 79% and the negative predictive value (NPV) was 100%. On the other hand, the professionals correctly predicted the outcome in 14 out of 20 trials,  attaining a sensitivity of 82% and specificity of 55%. The PPV is 69% and the NPV is 71%.\n\n\nFurthermore, SOCALNet also outperformed human experts in predicting blood loss. The experts systematically underestimate blood loss, with a mean error of -131 mL and a root mean square error (RMSE) of 350 mL. SOCALNet has a mean error of -57 mL and an RMSE of 295 mL. SOCALNet detects most episodes of blood loss greater than 500 mL (80%), while the human experts identify less than half of such episodes (47.5%).\n\n\nThe study shows that both  human experts and SOCALNet are capable of predicting outcome and blood loss during surgical hemorrhage based on first minute of the video taking individual frames into consideration. \nHowever, what's important to note here is the superior performance of SOCALNet over human experts. \nDeep learning algorithms have the potential to provide accurate assessments of surgical video and call for the creation of datasets of surgical adverse events for further research in quality improvement.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"The integration of artificial intelligence (AI) in surgery has the potential to revolutionize the field and improve patient outcomes. With the inherent risks and complexities associated with surgery, leveraging AI can help mitigate these risks and enhance patient safety throughout all phases of the surgical process. The potential of AI in surgery has gained significant attention, with growing interest and investment in this transformative technology. The AI in surgery market is expected to reach substantial figures in the coming years, indicating the recognition of its value in improving surgical outcomes. From screening and diagnosis to operative planning and postoperative assessment, AI algorithms can assist in various aspects of brain tumor management. With advancements in image-guided surgery, augmented reality visualization, and automated pathological assessment, AI can enhance surgical precision, improve postoperative analysis, and aid in personalized patient monitoring.Furthermore, recent advancements in autonomous robots, such as the STAR robot developed by researchers at Johns Hopkins University, showcase the increasing autonomy and capabilities of AI in surgery. These robots can track tissue movement, detect tool failures, adjust camera views, suggest suture plans, and predict tool collisions, all of which contribute to safer and more efficient surgical procedures.While AI in surgery holds immense promise, it is important to note that human expertise and judgment remain crucial. AI should be seen as a tool to augment and support healthcare professionals rather than replace them. The collaboration between AI systems and surgical teams can lead to a new era of precision medicine and improved patient care.\nIn conclusion, as we continue to embrace technological advancements, the integration of AI in surgery has the potential to reshape the landscape of medicine, making surgeries safer, more precise, and ultimately saving more lives.","metadata":{}},{"cell_type":"markdown","source":"# <center> PART II : AI in Drug Discovery </center>","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src=\"https://venturebeat.com/wp-content/uploads/2021/07/Proteins_-A0A143ZZK9_Malaria_Parasite.jpg?fit=750%2C422&strip=all\" height=\"900\" width=\"900\"/></center>\n<br>\n<center>Figure - 6: Image Source: <i><a href=\"https://venturebeat.com/ai/3-ai-trends-in-drug-discovery-that-stood-out-in-2022/\">DeepMind.</a></i></center>\n<br>\n","metadata":{}},{"cell_type":"markdown","source":"Drug discovery is a complex and an intensive process typically taking several years and involving significant financial investment. However, recent advancements in artificial intelligence (AI) are revolutionizing every stage of drug discovery, offering immense potential to transform the speed and economics of the pharmaceutical industry.\n\nTarget identification : Training on large datasets of biological information( including omics data, disease associations, patents, clinical trials, and research grants) AI can gain a comprehensive understanding of the underlying mechanisms of diseases enabling the process of identification of novel proteins and genes that can be targeted to combat these diseases effectively. Also,  the discovery of tools like AlphaFold, makes it  possible to predict the 3D structures of potential targets, accelerating the design of drugs that can bind to these targets with high precision.\n\nMolecular simulations is another critical aspect of drug discovery where AI is used.  Using ML/DL algorithms to conduct molecular simulations entirely on computers, eliminating the need for costly and time-consuming physical testing of candidate drug compounds significantly reduce the financial burden associated with traditional chemistry methods.\n\nAI systems are also being leveraged to predict crucial properties of drug candidates, such as toxicity, bioactivity, and physicochemical characteristics. By employing machine learning algorithms trained on vast databases of existing drug compounds and their associated properties, AI can provide valuable insights into the potential efficacy and safety of new drug candidates. This predictive ability allows researchers to focus their resources on the most promising compounds, streamlining the drug discovery process.\n\nFurthermore, AI is shifting the paradigm of drug design itself. Instead of relying solely on screening large libraries of existing molecules, AI systems are capable of generating entirely new and promising drug candidates from scratch. By employing generative models and reinforcement learning techniques, AI can explore vast chemical space and propose novel molecular structures with desired properties. This capability opens up new avenues for drug discovery, potentially uncovering compounds that would have been overlooked using traditional approaches.\n\nOnce a set of promising lead compounds has been identified, AI plays a crucial role in candidate drug prioritization. By employing advanced algorithms, AI can rank and prioritize these molecules based on various factors such as efficacy, safety, and market potential. AI-based approaches have demonstrated superior performance compared to previous ranking techniques, allowing researchers to make informed decisions and allocate resources effectively.\n\nMoreover, AI is not limited to theoretical drug design but also contributes to the practical aspects of drug synthesis. AI-driven algorithms can generate synthesis pathways for producing hypothetical drug compounds, sometimes suggesting modifications to enhance their manufacturability. By optimizing the synthesis process, AI can contribute to cost reduction and streamline the production of new drugs.\n\nAs AI technologies continue to improve, the idea of fully automated end-to-end drug discovery becomes increasingly viable. With advancements in target identification, molecular simulations, drug property prediction, de novo drug design, candidate drug prioritization, and synthesis pathway generation, AI offers the potential for a more efficient and cost-effective drug discovery process. While challenges and limitations remain, the pharmaceutical industry is increasingly recognizing the transformative power of AI in reshaping the future of drug development.","metadata":{}},{"cell_type":"markdown","source":"# <center> Recent Advancements</center>","metadata":{}},{"cell_type":"markdown","source":"# 1. AlphaFold","metadata":{}},{"cell_type":"markdown","source":"\n<center><img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_Fig1_HTML.png?as=webp\" height=\"400\" width=\"800\"/></center>\n<br>\n<center>Figure - 7: Image Source: <i><a href=\"https://www.nature.com/articles/s41586-021-03819-2\">AlphaFold.</i></center>\n<br>\n\n","metadata":{}},{"cell_type":"markdown","source":"AlphaFold[[17]](https://www.nature.com/articles/s41586-021-03819-2), the deep learning system developed by DeepMind, has not only revolutionized protein structure prediction but also provided valuable insights and lessons for the machine learning (ML) community. Its breakthrough achievements have highlighted the potential of deep learning in addressing complex scientific challenges, and the ML community has eagerly embraced the lessons learned from AlphaFold's success. The model can predict the 3D structure of the protein surpassing the traditional approaches / efforts made. By understanding protein structure/folding, one can understand protein function helping reseachers gain insights of the diseases associated with misfolded proteins. This can also help scientists recognizing protein function involved in biological processes. This would accelerate drug development by identifying potential drug target and designing drug that accurately binds to those drug targets.\n\n**Diving into the architecture**\n\n\nAlphaFold employs a deep neural network architecture that consists of multiple layers of interconnected artificial neurons. The neural network architecture used in AlphaFold is based on a variant of the attention-based Transformer model, originally developed for natural language processing tasks. The Transformer model has been adapted to process protein sequence data and make predictions about their 3D structures.\n\nInput Encoding:\nThe protein sequence is first encoded into a numerical representation that can be understood by the neural network. One common encoding scheme is one-hot encoding, where each amino acid in the sequence is represented as a binary vector of zeros except for the position representing the specific amino acid, which is set to one. The encoded sequence is then passed through the neural network for further processing.\n\nAttention Mechanism:\nIt is a key component in AlphaFold's Transformer model allowing the model weigh the importance of different parts of the input sequence when predictions are made. \n( in this case attention helps to capture long-range dependencies between  amino acids in the sequence, which is crucial for predicting the 3D structure accurately)\n\n\nMulti-Head Attention:\nAlphaFold utilizes multi-head attention, where multiple attention heads are used to capture different aspects of the input sequence. Each attention head attends to different parts of the sequence independently, allowing the model to capture diverse and complementary information. The outputs from multiple attention heads are then combined to produce a more comprehensive representation.\n\nResidual Connections and Layer Normalization:\nTo facilitate the flow of information and alleviate the vanishing gradient problem, residual connections are employed in the neural network architecture. These connections allow the model to retain information from earlier layers and make it easier to propagate gradients during training. Layer normalization is also applied to ensure stable and efficient training of the neural network.\n\nTraining with Distance Constraints:\nDuring the training process, AlphaFold utilizes distance constraints derived from experimental data to guide the learning process. These constraints provide additional information about the spatial relationships between amino acids in the 3D structure. By incorporating these constraints into the training process, the model can learn to predict protein structures that align with the experimental data.\n\nEnd-to-End Training:\nAlphaFold is trained in an end-to-end manner, meaning that the entire neural network is trained together rather than training individual components separately. This allows the model to learn complex patterns and relationships directly from the input protein sequences, optimizing its performance on the task of protein structure prediction.\n\nModel Optimization:\nThe training process involves optimizing the model's parameters to minimize the difference between the predicted protein structures and the experimental structures. \n\nTransfer Learning:\nAlphaFold leverages transfer [[18]](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8) learning to benefit from knowledge learned from previously solved protein structures. The model is initially pretrained on a large dataset of protein structures to learn general representations and patterns. This pretrained model is then fine-tuned on specific tasks, such as predicting the structures of newly observed proteins. Transfer learning enables the model to generalize well to unseen data and improves its prediction accuracy.\n\n**Key Takeaways**\n\n\n1. Importance of Data Availability:\nAlphaFold's success has underscored the critical role of data availability in training effective deep learning models. DeepMind trained AlphaFold on vast amounts of protein sequence and structural data, allowing the model to learn intricate relationships and patterns within the data. We have recognized the importance of curated, diverse, and high-quality datasets for training robust models and has made efforts to create and share similar resources for other domains.\n\n2. Interdisciplinary Collaboration:\nThe development of AlphaFold required close collaboration between ML researchers and experts in biology and biochemistry. DeepMind collaborated with protein structure experts to refine the model's training process and validate its predictions. This interdisciplinary collaboration has highlighted the importance of domain expertise and cross-disciplinary knowledge sharing for advancing ML applications in scientific domains. The community has embraced this lesson and has started fostering collaborations with domain experts to tackle complex challenges effectively.\n\n3. Explainability and Interpretability:\nAlphaFold's success has prompted discussions within the community regarding the explainability and interpretability of deep learning models. While AlphaFold's predictions have demonstrated remarkable accuracy, understanding how the model arrives at those predictions remains a challenge. Researchers have recognized the need for developing interpretability techniques to uncover the underlying mechanisms of deep learning models, enabling scientists to trust and gain insights from AI systems in scientific research.\n\n4. Accelerating Scientific Discovery:\nAlphaFold's breakthrough in protein structure prediction has inspired the ML community to explore its potential to accelerate scientific discovery in other domains. Researchers are actively investigating the application of deep learning models in drug discovery, materials science, genomics, and other areas where understanding complex molecular structures is crucial. AlphaFold's success has served as a catalyst for ML researchers to apply similar techniques to solve critical scientific problems and drive innovation.\n\n\nThe transferability of deep learning models, the importance of data availability, interdisciplinary collaboration, and the need for explainability and interpretability have emerged as key takeaways from AlphaFold's success. As we continue to build upon these lessons, we can expect to witness further advancements that address complex scientific challenges and drive interdisciplinary research collaborations, leading to accelerated scientific discovery.","metadata":{}},{"cell_type":"markdown","source":"# 2. Absci First to Create and Validate De Novo Antibodies with Zero-Shot Generative AI. \n","metadata":{}},{"cell_type":"markdown","source":"\nAbsci Corporation, a generative AI drug creation company, has achieved a significant milestone in the biotechnology industry by using zero-shot generative AI [[19]](https://www.biorxiv.org/content/10.1101/2023.01.08.523187v3.full) to create and validate de novo antibodies in silico. This breakthrough has the potential to revolutionize the process of antibody design and accelerate the time it takes to bring new drugs to the clinic while increasing their probability of success.\n\nIt typically takes over 10 years and more than $1 billion to bring a single drug to market. This limits the number of treatments that can be pursued by drugmakers. Absci's breakthrough in creating de novo antibodies using generative AI represents a significant step toward overcoming these challenges and delivering breakthrough therapeutics efficiently[[19]](https://www.biorxiv.org/content/10.1101/2023.01.08.523187v3.full).\n\nZero-Shot Generative AI for Antibody Design:\nAbsci uses zero-shot generative AI, a method that involves de novo antibody design that binds to specific targets. The catch here is, it does not  rely on training data from known antibodies. This approach allows for the creation of novel antibody designs that differ from those found in existing antibody databases. Remarkably, the generated designs demonstrated efficacy in the laboratory without the need for additional optimization steps\n\nAccelerating Drug Discovery Process:\nGenerating de novo antibodies using Zero Shot can  reduce the time required to bring new drug leads into the clinic (from six years to just 18-24 months) and increase the probability of success for novel therapeutics[[19]](https://www.biorxiv.org/content/10.1101/2023.01.08.523187v3.full).\n\nAddressing the Challenges of Generative AI:\nThe availability of scalable biological data remains to be one of the main obstacles in applying generative AI to biologic drug discovery. Absci has overcome this challenge by developing a proprietary high-throughput wet lab technology. This technology enables the testing and validation of nearly 3 million unique AI-generated designs per week[[19]](https://www.biorxiv.org/content/10.1101/2023.01.08.523187v3.full).. The data generated in the wet lab serves as a crucial component for refining generative AI models and improving antibody designs.\n\nExpanding Therapeutic Possibilities:\nAbsci's breakthrough not only accelerates the drug discovery process but also opens doors to addressing diseases previously deemed \"undruggable.\" Integrating generative AI with innovative synthetic biology, Absci aims to revolutionize medicine and improve therapeutic options for patients.[[19]](https://www.biorxiv.org/content/10.1101/2023.01.08.523187v3.full).\n\nAbsci's achievement in creating and validating de novo antibodies with zero-shot generative AI represents a significant breakthrough in the biotechnology industry. By leveraging the power of generative AI, Absci aims to accelerate the time to clinic, increase the success rate of drug development, and unlock transformative therapies for patients. This milestone paves the way for a future where breakthrough therapeutics can be designed and delivered at the click of a button, revolutionizing healthcare.\n[[19]](https://www.biorxiv.org/content/10.1101/2023.01.08.523187v3.full).\n","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"In conclusion, recent advancements in artificial intelligence (AI) have the potential to revolutionize the entire process of drug discovery. From target identification to molecular simulations, and from drug property prediction to candidate prioritization, AI is transforming every stage of the pharmaceutical industry. AI algorithms are capable of analyzing vast datasets and understanding the underlying mechanisms of diseases, leading to the identification of novel targets for effective drug development. Molecular simulations conducted entirely on computers, guided by AI algorithms, reduce the need for costly physical testing of drug compounds. AI also aids in predicting crucial properties of drug candidates, allowing researchers to focus their resources on the most promising compounds. Additionally, AI systems can generate entirely new drug candidates from scratch, expanding the possibilities of drug design. Furthermore, AI contributes to the practical aspects of drug synthesis, optimizing the production of new drugs and reducing costs. As AI technologies continue to improve, the concept of fully automated end-to-end drug discovery becomes increasingly feasible, promising a more efficient and cost-effective process. While challenges and limitations remain, the pharmaceutical industry recognizes the transformative power of AI in reshaping the future of drug development. The achievements of AI systems like AlphaFold and Absci's generative AI in protein structure prediction and antibody design, respectively, showcase the tremendous potential and impact of AI in the biotechnology field. These breakthroughs not only accelerate the drug discovery process but also open doors to addressing previously deemed \"undruggable\" diseases and improving therapeutic options for patients. As we embrace the power of AI, we move closer to a future where breakthrough therapeutics can be designed and delivered rapidly, transforming healthcare and improving lives.","metadata":{}},{"cell_type":"markdown","source":"# <CENTER> PART III : Kaggle Healthcare Competitions Summary <CENTER>","metadata":{}},{"cell_type":"markdown","source":"In the last two years, Kaggle, has launched a total of 163 competitions[[21]](https://www.kaggle.com/datasets/kaggle/meta-kaggle). Among these competitions, 18 were specifically focused on healthcare-related challenges. These competitions aimed to tackle various problems and advance knowledge in the healthcare field using data-driven approaches. Let's explore each of these healthcare competitions in more detail:\n\n**HuBMAP - Hacking the Human Vasculature**:\nThis competition aimed to develop models for identifying and mapping the intricate network of blood vessels within human tissue samples.\n\n**ICR - Identifying Age-Related Conditions**:\nParticipants were tasked with predicting the presence of age-related conditions using anonymized patient data, including medical history, diagnoses, and demographic information.\n\n**CAFA 5 Protein Function Prediction**:\nThe goal of this competition was to improve the prediction of protein functions, which is crucial for understanding the biological mechanisms underlying diseases and developing new treatments.\n\n**Parkinson's Freezing of Gait Prediction**:\nParticipants were challenged to create algorithms that could accurately predict episodes of freezing of gait in Parkinson's disease patients based on wearable sensor data.\n\n**AMP®-Parkinson's Disease Progression Prediction**:\nThis competition focused on predicting the progression of Parkinson's disease in patients using diverse data sources, including clinical assessments, genetic information, and biomarkers.\n\n**RSNA Screening Mammography Breast Cancer Detection**:\nThe objective of this competition was to develop machine learning models capable of accurately detecting breast cancer in screening mammography images, aiming to improve early diagnosis.\n\n**Novozymes Enzyme Stability Prediction**:\nParticipants were asked to predict the stability of enzymes used in industrial applications, helping to optimize their performance and enhance the development of new biotechnological processes.\n\n**Open Problems - Multimodal Single-Cell Integration**:\nThis competition aimed to address the challenge of integrating different types of single-cell data, such as gene expression and epigenetic profiles, to gain a more comprehensive understanding of cellular processes.\n\n**RSNA 2022 Cervical Spine Fracture Detection**:\nThe task of this competition was to develop algorithms capable of accurately detecting cervical spine fractures in trauma patients using radiographic images.\n\n**Mayo Clinic - STRIP AI**:\nParticipants were presented with electronic health records from patients undergoing cardiac surgery and were tasked with predicting the likelihood of developing postoperative complications.\n\n**HuBMAP + HPA - Hacking the Human Body**:\nThis competition sought to explore the integration of imaging data from the Human Biomolecular Atlas Program (HuBMAP) and protein expression data from the Human Protein Atlas (HPA) to gain insights into human biology.\n\n**UW-Madison GI Tract Image Segmentation**:\nThe goal of this competition was to develop models for segmenting gastrointestinal tract structures in endoscopic images, enabling more accurate diagnoses and treatment planning.\n\n**NBME - Score Clinical Patient Notes**:\nParticipants were challenged to predict clinical scores based on patient notes, aiming to provide insights into the relationship between clinical documentation and performance on standardized exams.\n\n**Sartorius - Cell Instance Segmentation**:\nThis competition focused on developing algorithms capable of segmenting individual cells within microscopic images, aiding in various biological research and medical applications.\n\n**Google Brain - Ventilator Pressure Prediction**:\nThe task of this competition was to predict ventilator airway pressure trajectories for intensive care patients, which could assist in optimizing ventilation strategies and improving patient outcomes.\n\n**SIIM-FISABIO-RSNA COVID-19 Detection**:\nGiven chest X-ray and CT scan images, participants were challenged to develop models that could accurately detect and classify COVID-19 cases, aiding in the diagnosis and management of the disease.\n\n**Human Protein Atlas - Single Cell Classification**:\nThis competition aimed to develop models for classifying subcellular protein patterns in microscopy images, facilitating the understanding of cellularfunctions and protein localization, which can have implications for various diseases and drug discovery.\n\n**HuBMAP - Hacking the Kidney**:\nThe goal of this competition was to develop models for segmenting and characterizing different structures within kidney tissue samples, contributing to the understanding of kidney function and disease.\n\nSeveral common and distinct tasks emerged, each presenting its own challenges and opportunities. \nLet's explore the types of tasks that were more prevalent, highlight the unique competitions, and discuss the challenges faced in these competitions.\n\n**1. Image Classification and Detection**\nSeveral competitions focused on image classification and detection tasks. Examples include the RSNA Screening Mammography Breast Cancer Detection, SIIM-FISABIO-RSNA COVID-19 Detection, and Human Protein Atlas - Single Cell Classification competitions. Participants were tasked with developing models to classify mammography images, detect COVID-19 in chest X-rays and CT scans, and classify protein patterns in microscopy images.\nChallenges: Handling the complexity and variability of medical images, addressing class imbalance, interpreting and explaining model predictions, and developing robust models that generalize well to unseen data.\n\n**2. Segmentation and Localization**\nCompetitions such as HuBMAP - Hacking the Human Vasculature, UW-Madison GI Tract Image Segmentation, and Sartorius - Cell Instance Segmentation required participants to develop models for accurate segmentation and localization of specific structures within medical images. Tasks included segmenting blood vessels, gastrointestinal tract regions, and individual cells in microscopy images.\nChallenges: Dealing with fine-grained structures, handling variations in shape, size, and appearance, developing precise and efficient segmentation algorithms, and addressing class imbalance within segmented regions.\n\n**3. Disease Prediction and Diagnosis**\nSeveral competitions focused on predicting or diagnosing specific diseases. Examples include ICR - Identifying Age-Related Conditions, Parkinson's Freezing of Gait Prediction, and Mayo Clinic - STRIP AI competitions. Participants were tasked with developing models to predict age-related conditions, freezing of gait in Parkinson's disease patients, and predicting stroke risk.\nChallenges: Identifying relevant features and biomarkers, dealing with heterogeneous and multimodal data, handling missing data, and developing models that are interpretable and explainable.\n\n**4. Function Prediction and Progression**\nCompetitions like CAFA 5 Protein Function Prediction and AMP®-Parkinson's Disease Progression Prediction required participants to predict protein function and disease progression, respectively. These tasks involved analyzing molecular and clinical data to make predictions about protein functions or disease progression trajectories.\nChallenges: Integrating diverse data sources, incorporating temporal information, addressing missing data, handling high-dimensional and noisy data, and developing models that capture complex relationships.\n\n**Unique Competitions**\n\n**a) Novozymes Enzyme Stability Prediction**: This competition focused on predicting enzyme stability, which is critical for industrial applications. Participants developed models to predict the stability of enzymes under different conditions, enabling the optimization of industrial processes.\n\n**b) Open Problems - Multimodal Single-Cell Integration**: This competition aimed to address the challenge of integrating multimodal single-cell data, such as gene expression, protein expression, and imaging data. Participants developed techniques to effectively integrate and analyze diverse single-cell modalities, enabling a more comprehensive understanding of cellular biology.\n\n**Challenges Faced**\n\n1. Limited Labeled Data: Many competitions faced the challenge of limited labeled data, especially in the medical domain where acquiring labeled data can be time-consuming and expensive. Participants had to develop effective strategies for data augmentation, transfer learning, and leveraging unlabeled data to address this challenge.\n\n2. Data Heterogeneity: Healthcare data is inherently diverse and heterogeneous, consisting of different data modalities, formats, and sources. Participants had to develop methods to handle and integrate various types of data, such as images, clinical measurements, genetic information, and textual data.\n\n3. Ethical Considerations: Healthcare competitions raise important ethical considerations, including patient privacy, data anonymization, and potential biases. Participants had to navigate these ethical challenges and ensure that their models and approaches adhere to privacy regulations and promote fairand unbiased healthcare outcomes.\n\n4. Interpretability and Explainability: Healthcare applications require models that are interpretable and provide explanations for their predictions. Participants had to develop techniques to interpret and explain model decisions, providing insights into feature importance and clinical relevance. This helped ensure that the models' predictions could be trusted and understood by medical professionals.\n\n5. Generalization to Unseen Data: Healthcare models should be able to generalize well to unseen data and be robust to variations in input conditions. Participants had to address challenges related to overfitting, model generalization, and robustness. Techniques such as cross-validation, data augmentation, and adversarial validation were employed to enhance model performance on unseen data.\n\n6. Clinical Relevance: Understanding the clinical context and relevance of the predicted outcomes was crucial in healthcare competitions. Participants needed to collaborate with domain experts and healthcare professionals to ensure that their models' predictions aligned with clinical knowledge and could be effectively integrated into healthcare workflows.\n\nIn conclusion, the healthcare competitions launched by Kaggle in the past two years covered a range of tasks, including image classification, segmentation, disease prediction, and function prediction. These competitions presented unique challenges related to data heterogeneity, limited labeled data, ethical considerations, interpretability, generalization, and clinical relevance. Overcoming these challenges has contributed to the advancement of ML architectures, algorithms, and domain-specific knowledge in the medical field, bringing us closer to more accurate and impactful healthcare solutions.These 18 healthcare-focused competitions organized by Kaggle demonstrate the platform's commitment to leveraging data science and machine learning to tackle important challenges in the healthcare domain. By harnessing the power of the Kaggle community, these competitions have contributed to advancements in medical research, diagnosis, treatment, and overall healthcare outcomes.\n","metadata":{}},{"cell_type":"markdown","source":"# AI Content Policy ","metadata":{}},{"cell_type":"markdown","source":"Generative AI (ChatGPT) was utilized at multiple stages of this essay writing.\n1. Before starting to write the article, given Healthcare is such a vast field, I put up some suggestions and was confused what all should I cover.The tool helped me decide by suggesting to focus on a few and go in-depth rather than trying to cover it all. \n2. When going through the research papers it was difficult to assess what info is cluttered and needs to be taken out and what to keep, the tool helped me suggest the information that was not needed to provide what was necessary to keep the essay concise. \n3. After preparing my first draft and multiple drafts later on, I used ChatGPT to ask the potential limitations of the current version that could help improve the essay. The suggestions helped me improve my essay further. \n4. During the entire process, I asked ChatGPT to be one of my reviewer, and it indeed took the job seriously. \n\nAt the end, I would like to specify, that no wrong/additional facts are generated from ChatGPT. The ChatGPT did not add anything in this essay from its own research.","metadata":{}},{"cell_type":"markdown","source":"# <center> References </center>","metadata":{}},{"cell_type":"markdown","source":"1. [AI in Surgery: Opportunities, Applications, and Challenges](https://nextgeninvent.com/blogs/ai-in-surgery-opportunities-applications-and-challenges/).\n2. [World Health Organization](https://www.who.int/).\n3. [The top 10 causes of death](https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death).\n4. [Coronavirus disease (COVID-19)](https://www.who.int/health-topics/coronavirus#tab=tab_1).\n5. [Diagnosing brain tumours by routine blood tests using machine learning](https://www.nature.com/articles/s41598-019-51147-3).\n6. [An AI-Powered Blood Test to Detect Cancer Using NanoDSF](https://www.nature.com/articles/s41598-019-51147-3).\n7. [Using machine learning for sequence-level automated MRI protocol selection in neuroradiology](https://academic.oup.com/jamia/article/25/5/568/4569611?login=false).\n8. [MRI radiomics analysis of molecular alterations in low-grade gliomas](https://link.springer.com/article/10.1007/s11548-017-1691-5).\n9. [A Cascaded Deep Convolutional Neural Network for Joint Segmentation and Genotype Prediction of Brainstem Gliomas\n](https://ieeexplore.ieee.org/document/8375811).\n10. [Autonomous robotic laparoscopic surgery for intestinal anastomosis](https://www.science.org/doi/10.1126/scirobotics.abj2908).\n11. [Understanding of a convolutional neural network](https://ieeexplore.ieee.org/document/8308186).\n12. [JHU-ISI Gesture and Skill Assessment Working\nSet (JIGSAWS): A Surgical Activity Dataset for\nHuman Motion Modeling](https://cirl.lcsr.jhu.edu/wp-content/uploads/2015/11/JIGSAWS.pdf).\n13. [Objective structured assessment of technical skill (OSATS) for surgical residents](https://pubmed.ncbi.nlm.nih.gov/9052454/).\n14. [Sparse Hidden Markov Models for Surgical Gesture Classification and Skill Evaluation](https://link.springer.com/chapter/10.1007/978-3-642-30618-1_17).\n15. [Automation of surgical skill assessment using a three-stage machine learning algorithm](https://www.nature.com/articles/s41598-021-84295-6).\n16. [Expert surgeons and deep learning models can predict the outcome of surgical hemorrhage from 1 min of video](https://www.nature.com/articles/s41598-022-11549-2).\n17. [Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2).\n18. [Modeling aspects of the language of life through transfer-learning protein sequences](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8).\n19. [Unlocking de novo antibody design with generative artificial intelligence](https://www.biorxiv.org/content/10.1101/2023.01.08.523187v3.full).\n20. [Failure Rayes in Surgery](https://medium.com/thinq-at-ucla/failure-rates-in-surgery-89ca73250b57).\n21. [Kaggle Write Ups](https://www.kaggle.com/datasets/kaggle/meta-kaggle).\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nsubmission = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\")\nsubmission.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:34:05.020144Z","iopub.execute_input":"2023-07-16T21:34:05.020875Z","iopub.status.idle":"2023-07-16T21:34:05.061611Z","shell.execute_reply.started":"2023-07-16T21:34:05.020813Z","shell.execute_reply":"2023-07-16T21:34:05.060426Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"             type                                              value\n0  essay_category  'copy/paste the exact category that you are su...\n1       essay_url  'http://www.kaggle.com/your_username/your_note...\n2   feedback1_url  'http://www.kaggle.com/.../your_1st_peer_feedb...\n3   feedback2_url  'http://www.kaggle.com/.../your_2nd_peer_feedb...\n4   feedback3_url  'http://www.kaggle.com/.../your_3rd_peer_feedb...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>essay_category</td>\n      <td>'copy/paste the exact category that you are su...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>essay_url</td>\n      <td>'http://www.kaggle.com/your_username/your_note...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feedback1_url</td>\n      <td>'http://www.kaggle.com/.../your_1st_peer_feedb...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feedback2_url</td>\n      <td>'http://www.kaggle.com/.../your_2nd_peer_feedb...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feedback3_url</td>\n      <td>'http://www.kaggle.com/.../your_3rd_peer_feedb...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"review = [\"Other\", \"https://www.kaggle.com/code/ahsuna123/kaggle-ai-report-healthcare-surge\",\n      \"https://www.kaggle.com/code/cynthiaaniobi/ai-in-medicine/comments#2344211\",\n      \"https://www.kaggle.com/code/diegoexe/kaggle-ai-report-medical-data/comments#2346179\",\n      \"https://www.kaggle.com/code/abdonasser/ai-in-healthcare/comments#2346749\"]\nsubmission.value = review\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:34:05.063933Z","iopub.execute_input":"2023-07-16T21:34:05.064272Z","iopub.status.idle":"2023-07-16T21:34:05.079750Z","shell.execute_reply.started":"2023-07-16T21:34:05.064242Z","shell.execute_reply":"2023-07-16T21:34:05.078729Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T21:34:05.081392Z","iopub.execute_input":"2023-07-16T21:34:05.081963Z","iopub.status.idle":"2023-07-16T21:34:05.094264Z","shell.execute_reply.started":"2023-07-16T21:34:05.081925Z","shell.execute_reply":"2023-07-16T21:34:05.093098Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"             type                                              value\n0  essay_category                                              Other\n1       essay_url  https://www.kaggle.com/code/ahsuna123/kaggle-a...\n2   feedback1_url  https://www.kaggle.com/code/cynthiaaniobi/ai-i...\n3   feedback2_url  https://www.kaggle.com/code/diegoexe/kaggle-ai...\n4   feedback3_url  https://www.kaggle.com/code/abdonasser/ai-in-h...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>essay_category</td>\n      <td>Other</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>essay_url</td>\n      <td>https://www.kaggle.com/code/ahsuna123/kaggle-a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feedback1_url</td>\n      <td>https://www.kaggle.com/code/cynthiaaniobi/ai-i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feedback2_url</td>\n      <td>https://www.kaggle.com/code/diegoexe/kaggle-ai...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feedback3_url</td>\n      <td>https://www.kaggle.com/code/abdonasser/ai-in-h...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}